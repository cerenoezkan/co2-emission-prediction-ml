{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction of CO2 emissions from country-specific data\n",
    "# A Machine Learning project \n",
    "### by Vladislav Todorov\n",
    "\n",
    "***\n",
    "\n",
    "# Stage 1: Data cleaning and preparation\n",
    "\n",
    "***\n",
    "\n",
    "### Notebook Contents:\n",
    "0. Introduction - project and notebook summary, notes on the data source\n",
    "1. Notebook setup - libraries and data import\n",
    "2. Global data overview\n",
    "3. Definition of the initial project goals\n",
    "3. Data cleaning\n",
    "    - dealing with missing values\n",
    "    - transformation of the columns into a numerical data type\n",
    "    - renaming of features\n",
    "    - removing empty columns and rows\n",
    "4. Data frame transformation\n",
    "    - melting of the data for each variable\n",
    "    - integration of the data into a suitable data frame format\n",
    "5. Removal of missing values\n",
    "    - detection of missing values\n",
    "    - removal of missing values by filtering the columns and rows, so that minimal amount of features and rows are lost\n",
    "5. Export the clean data frame to a file\n",
    "\n",
    "***\n",
    "\n",
    "## 0. Introduction\n",
    "\n",
    "### Project summary\n",
    "**Aim of the project**:\n",
    "Analysis of country-specific data and development of machine learning models in order to predict CO2 emissions from country parameters. The project uses the publicly available dataset Climate Change Data from the World Bank Group, which provides data on the vast majority of countries over a range of years for parameters such as:\n",
    "\n",
    "* country: the vast majority of countries worldwide\n",
    "* year: ranging from 1990 to 2011\n",
    "* various emissions of greenhouse gases such as CO2, CH4, N2O, others\n",
    "* population-specific parameters: population count, urban population, population growth, etc.\n",
    "* country economic indicators: GDP, GNI, Foreign Direct Investment, etc.\n",
    "* land-related parameters: cereal yield, agricultural land, Nationally terrestrial protected areas, etc.\n",
    "* climate data: precipitations, national disasters, etc.\n",
    "* energy use\n",
    "* counts of certain types of medical personnel\n",
    "* etc.\n",
    "\n",
    "The project is divided into three stages:\n",
    "\n",
    "1. Data cleaning and preparation\n",
    "2. Data exploration and visualization\n",
    "3. Predictive analysis with the Random Forest machine learning algorithm\n",
    "\n",
    "Each of the stages is described in a separate Jupyter Notebook(.ipynp file) and a derived pdf file.\n",
    "\n",
    "***\n",
    "\n",
    "### Notebook summary - Stage 1: Data cleaning and preparation\n",
    "**Aim of this notebook**: The subject of this particular notebook is to explain the first stage of the project - the cleaning and transformation of the available data in order to prepare it for the visualization and analysis (described in further notebooks).\n",
    "\n",
    "**Input**: excel data file from the original online source\n",
    "\n",
    "**Output**: comma separated values (CSV) file containing the cleaned data ready for visualization and analysis\n",
    "\n",
    "**Programming language**: Python 3.7\n",
    "\n",
    "**Libraries used in this notebook**: pandas, numpy\n",
    "\n",
    "***\n",
    "\n",
    "### Data source\n",
    "\n",
    "The used data comes from the Climate Change Data of the World Bank Group, which provides country-specific data on parameters such as CO2 emissions, energy use, population count, urban population, cereal yield, nationally terrestrial protected areas, GDP, GNI, etc.\n",
    "\n",
    "\n",
    "The dataset is publicly available at https://datacatalog.worldbank.org/dataset/climate-change-data and licenced under the <a href=\"https://datacatalog.worldbank.org/public-licenses#cc-by\">Creative Commons Attribution 4.0 International license</a>.\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Notebook Setup\n",
    "Import all needed libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complete Climate Change Dataset is imported into a pandas DataFrame from the downloaded file \"climate_change_download_0.xls\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Missing optional dependency 'xlrd'. Install xlrd >= 2.0.1 for xls Excel support Use pip or conda to install xlrd.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\Lib\\site-packages\\pandas\\compat\\_optional.py:135\u001b[39m, in \u001b[36mimport_optional_dependency\u001b[39m\u001b[34m(name, extra, errors, min_version)\u001b[39m\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m     module = \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\Lib\\importlib\\__init__.py:88\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m     87\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1387\u001b[39m, in \u001b[36m_gcd_import\u001b[39m\u001b[34m(name, package, level)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1360\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1324\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'xlrd'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m data_sheet = \u001b[33m\"\u001b[39m\u001b[33mData\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# read the data from the excel file to a pandas DataFrame\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m data_orig = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mio\u001b[49m\u001b[43m=\u001b[49m\u001b[43morig_data_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_sheet\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:495\u001b[39m, in \u001b[36mread_excel\u001b[39m\u001b[34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[39m\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[32m    494\u001b[39m     should_close = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m495\u001b[39m     io = \u001b[43mExcelFile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine != io.engine:\n\u001b[32m    502\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    503\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mEngine should not be specified when passing \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    504\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    505\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1567\u001b[39m, in \u001b[36mExcelFile.__init__\u001b[39m\u001b[34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[39m\n\u001b[32m   1564\u001b[39m \u001b[38;5;28mself\u001b[39m.engine = engine\n\u001b[32m   1565\u001b[39m \u001b[38;5;28mself\u001b[39m.storage_options = storage_options\n\u001b[32m-> \u001b[39m\u001b[32m1567\u001b[39m \u001b[38;5;28mself\u001b[39m._reader = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engines\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1568\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_io\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1569\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1570\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1571\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\excel\\_xlrd.py:45\u001b[39m, in \u001b[36mXlrdReader.__init__\u001b[39m\u001b[34m(self, filepath_or_buffer, storage_options, engine_kwargs)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[33;03mReader using xlrd engine.\u001b[39;00m\n\u001b[32m     35\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     42\u001b[39m \u001b[33;03m    Arbitrary keyword arguments passed to excel engine.\u001b[39;00m\n\u001b[32m     43\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     44\u001b[39m err_msg = \u001b[33m\"\u001b[39m\u001b[33mInstall xlrd >= 2.0.1 for xls Excel support\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m \u001b[43mimport_optional_dependency\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mxlrd\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra\u001b[49m\u001b[43m=\u001b[49m\u001b[43merr_msg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\n\u001b[32m     47\u001b[39m     filepath_or_buffer,\n\u001b[32m     48\u001b[39m     storage_options=storage_options,\n\u001b[32m     49\u001b[39m     engine_kwargs=engine_kwargs,\n\u001b[32m     50\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\Lib\\site-packages\\pandas\\compat\\_optional.py:138\u001b[39m, in \u001b[36mimport_optional_dependency\u001b[39m\u001b[34m(name, extra, errors, min_version)\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m    137\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m errors == \u001b[33m\"\u001b[39m\u001b[33mraise\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[38;5;66;03m# Handle submodules: if we have submodule, grab parent module from sys.modules\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: Missing optional dependency 'xlrd'. Install xlrd >= 2.0.1 for xls Excel support Use pip or conda to install xlrd."
     ]
    }
   ],
   "source": [
    "# define the file name and the data sheet\n",
    "orig_data_file = r\"climate_change_download_0.xls\"\n",
    "data_sheet = \"Data\"\n",
    "\n",
    "# read the data from the excel file to a pandas DataFrame\n",
    "data_orig = pd.read_excel(io=orig_data_file, sheet_name=data_sheet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## 2. Global data overview\n",
    "\n",
    "A global overview of the imported data yields the following insights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of the original dataset:\")\n",
    "data_orig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Available columns:\")\n",
    "data_orig.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Column data types:\")\n",
    "data_orig.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overview of the first 5 rows:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'data_orig' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mOverview of the first 5 rows:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mdata_orig\u001b[49m.head()\n",
      "\u001b[31mNameError\u001b[39m: name 'data_orig' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Overview of the first 5 rows:\")\n",
    "data_orig.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptive statistics of the columns:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'data_orig' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mDescriptive statistics of the columns:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mdata_orig\u001b[49m.describe()\n",
      "\u001b[31mNameError\u001b[39m: name 'data_orig' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Descriptive statistics of the columns:\")\n",
    "data_orig.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to better understand the nature of the columns \"Series code\", \"Series name', \"SCALE\" and \"Decimals\", it is necessary to examine their values.\n",
    "\n",
    "The following snippet prints the contents of the column *'Series name'*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_orig' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdata_orig\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mSeries name\u001b[39m\u001b[33m'\u001b[39m].unique()\n",
      "\u001b[31mNameError\u001b[39m: name 'data_orig' is not defined"
     ]
    }
   ],
   "source": [
    "data_orig['Series name'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contents of the column *'Series code'*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_orig' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdata_orig\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mSeries code\u001b[39m\u001b[33m'\u001b[39m].unique()\n",
      "\u001b[31mNameError\u001b[39m: name 'data_orig' is not defined"
     ]
    }
   ],
   "source": [
    "data_orig['Series code'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contents of the column *'SCALE'*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_orig' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdata_orig\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mSCALE\u001b[39m\u001b[33m'\u001b[39m].unique()\n",
      "\u001b[31mNameError\u001b[39m: name 'data_orig' is not defined"
     ]
    }
   ],
   "source": [
    "data_orig['SCALE'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contents of the column *'Decimals'*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_orig' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdata_orig\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mDecimals\u001b[39m\u001b[33m'\u001b[39m].unique()\n",
      "\u001b[31mNameError\u001b[39m: name 'data_orig' is not defined"
     ]
    }
   ],
   "source": [
    "data_orig['Decimals'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, it is unclear what are the values marked with the label *'Text'* in the *'SCALE'* and *'Decimals'* columns. These are shown in the following tow outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_orig' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdata_orig\u001b[49m[data_orig[\u001b[33m'\u001b[39m\u001b[33mSCALE\u001b[39m\u001b[33m'\u001b[39m]==\u001b[33m'\u001b[39m\u001b[33mText\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[31mNameError\u001b[39m: name 'data_orig' is not defined"
     ]
    }
   ],
   "source": [
    "data_orig[data_orig['SCALE']=='Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_orig' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdata_orig\u001b[49m[data_orig[\u001b[33m'\u001b[39m\u001b[33mDecimals\u001b[39m\u001b[33m'\u001b[39m]==\u001b[33m'\u001b[39m\u001b[33mText\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[31mNameError\u001b[39m: name 'data_orig' is not defined"
     ]
    }
   ],
   "source": [
    "data_orig[data_orig['Decimals']=='Text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Findings from the global overview\n",
    "\n",
    "This global overview gives away the following facts about the available data:\n",
    "* shape: 28 columns, 13512 rows\n",
    "* all columns are of type \"object\" - neither numeric, nor string/text values\n",
    "* A certain amount of missing values, denoted both as NaN (not a number values) and as the string \"..\"\n",
    "* The rows marked as *'Text'* in the columns *'SCALE'* and *'Decimals'* do not contain any information, almost completely composed of NaN values\n",
    "* The columns represent key values such as country, but also the corresponding years and the series code/name\n",
    "* The columns *'Country name'*, *'Series code'*, *'SCALE'* and *'Decimals'* do not give any information and are therefore obsolete\n",
    "* The column *'Series name'* contains the country-specific features required for the analysis\n",
    "* The names of the features in the column *'Series name'* are clear but too long\n",
    "\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define the initial project goals\n",
    "\n",
    "The first overview of the raw data allows to define initial goals and objectives of the machine learning project. These will be refined in the future as more information insight is gained from the data. However, this initial goal definition will help develop a strategy and organize the data cleaning, transformation and visualization.\n",
    "\n",
    "The data series available can be summarized into the following country-specific parameter/feature categories:\n",
    "\n",
    "* various emissions of greenhouse gases such as CO2, CH4, N2O, others\n",
    "* population-specific parameters: population count, urban population, population growth, etc.\n",
    "* country economic indicators: GDP, GNI, Foreign Direct Investment, etc.\n",
    "* land-related parameters: cereal yield, agricultural land, Nationally terrestrial protected areas, etc.\n",
    "* climate data: precipitations, national disasters, etc.\n",
    "* energy use\n",
    "* counts of certain types of medical personnel\n",
    "* etc.\n",
    "\n",
    "Such a dataset would suggest to investigate the influence of country-specific parameters such as economic parameres, population, energy use, land use and others on climate-related data or the factors affecting the climate like emissions, precipitations, etc.\n",
    "\n",
    "**Initial goal of the machine learning project:** Analyze the relationships among these variable categories and evaluate the contribution of factors like country economy, energy use, land use, etc. on greenhouse gas emissions, precipitations, etc. Finally, develop a machine learning model capable of predicting climate-related data or emissions from the other country-specific parameters.\n",
    "\n",
    "As more data insight will be gained with along the course of the project, the definition of these goals will be refined in more detail.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data cleaning\n",
    "\n",
    "### Organization of the data cleaning and transformation\n",
    "\n",
    "The main aim of the data cleaning and transformation is to represent the features (the country parameters contained in the column *'Series name'*) as separate columns and to make each row identifiable by a country and a year. At the same time, it would make sense to transform the years into a single column.\n",
    "\n",
    "Additionally, it is necessary to get rid of empty rows or columns and deal with the remaining cells with missing values.\n",
    "\n",
    "For these purposes, the following tasks have to be undertaken:\n",
    "\n",
    "1. Remove rows marked as \"Text\" in the \"SCALE\" and \"Decimals\" columns\n",
    "2. Remove the unnecessary columns \"Country name\", \"Series code\", \"SCALE\", \"Decimals\"\n",
    "3. Transform the \"..\" strings and emplty cells (\"\") into NaN values for easier recognission as missing values\n",
    "4. Transform all data columns into a numerical data type\n",
    "5. Rename the features in column \"Series name\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Removing rows marked as \"Text\" in the \"SCALE\" and \"Decimals\" columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_orig' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# assign the data to a new DataFrame, which will be modified\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m data_clean = \u001b[43mdata_orig\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mOriginal number of rows:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(data_clean.shape[\u001b[32m0\u001b[39m])\n",
      "\u001b[31mNameError\u001b[39m: name 'data_orig' is not defined"
     ]
    }
   ],
   "source": [
    "# assign the data to a new DataFrame, which will be modified\n",
    "data_clean = data_orig\n",
    "\n",
    "print(\"Original number of rows:\")\n",
    "print(data_clean.shape[0])\n",
    "\n",
    "# remove rows characterized as \"Text\" in the SCALE column\n",
    "data_clean = data_clean[data_clean['SCALE']!='Text']\n",
    "\n",
    "print(\"Current number of rows:\")\n",
    "print(data_clean.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Removing the unnecessary columns \"Country name\", \"Series code\", \"SCALE\", \"Decimals\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of columns:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'data_clean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mOriginal number of columns:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdata_clean\u001b[49m.shape[\u001b[32m1\u001b[39m])\n\u001b[32m      4\u001b[39m data_clean = data_clean.drop([\u001b[33m'\u001b[39m\u001b[33mCountry name\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mSeries code\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mSCALE\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mDecimals\u001b[39m\u001b[33m'\u001b[39m], axis=\u001b[33m'\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCurrent number of columns:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'data_clean' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Original number of columns:\")\n",
    "print(data_clean.shape[1])\n",
    "\n",
    "data_clean = data_clean.drop(['Country name', 'Series code', 'SCALE', 'Decimals'], axis='columns')\n",
    "\n",
    "print(\"Current number of columns:\")\n",
    "print(data_clean.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Transform the \"..\" strings and emplty cells (\"\") into NaN values for easier recognission as missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_clean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m data_clean.iloc[:,\u001b[32m2\u001b[39m:] = \u001b[43mdata_clean\u001b[49m.iloc[:,\u001b[32m2\u001b[39m:].replace({\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m:np.nan, \u001b[33m'\u001b[39m\u001b[33m..\u001b[39m\u001b[33m'\u001b[39m:np.nan})\n",
      "\u001b[31mNameError\u001b[39m: name 'data_clean' is not defined"
     ]
    }
   ],
   "source": [
    "data_clean.iloc[:,2:] = data_clean.iloc[:,2:].replace({'':np.nan, '..':np.nan})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Transform all data columns into a numerical data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_clean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m data_clean2 = \u001b[43mdata_clean\u001b[49m.applymap(\u001b[38;5;28;01mlambda\u001b[39;00m x: pd.to_numeric(x, errors=\u001b[33m'\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Errors are ignored in order to avoid error messages about the first two columns, which don't need to be transformed\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# into numeric type anyway\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPrint the column data types after transformation:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'data_clean' is not defined"
     ]
    }
   ],
   "source": [
    "data_clean2 = data_clean.applymap(lambda x: pd.to_numeric(x, errors='ignore'))\n",
    "# Errors are ignored in order to avoid error messages about the first two columns, which don't need to be transformed\n",
    "# into numeric type anyway\n",
    "\n",
    "print(\"Print the column data types after transformation:\")\n",
    "data_clean2.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Rename the features in column \"Series name\"\n",
    "\n",
    "The variable/feature names in the column *'Series name'* are clear, but too long and not practical to use in the code. In order to improve that, the most relevant feature names will be renamed with shorter labels as indicated in the following table:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td> <b>Variable name</b> </td>\n",
    "        <td> <b>Description</b> </td>\n",
    "        <td> <b>Unit</b> </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> cereal_yield </td>\n",
    "        <td> Cereal yield </td>\n",
    "        <td> kg per hectare </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> fdi_perc_gdp </td>\n",
    "        <td> Foreign direct investment, net inflows </td>\n",
    "        <td> % of GDP </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> elec_access_perc </td>\n",
    "        <td> Access to electricity </td>\n",
    "        <td> % of total population </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> en_per_gdp </td>\n",
    "        <td> Energy use per units of GDP </td>\n",
    "        <td> kg oil eq./\\$1,000 of 2005 PPP \\$ </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> en_per_cap </td>\n",
    "        <td> Energy use per capita </td>\n",
    "        <td> kilograms of oil equivalent </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> co2_ttl </td>\n",
    "        <td> CO2 emissions, total </td>\n",
    "        <td> KtCO2 </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> co2_per_cap </td>\n",
    "        <td> CO2 emissions, total </td>\n",
    "        <td> metric tons </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> co2_per_gdp </td>\n",
    "        <td> CO2 emissions per units of GDP </td>\n",
    "        <td> kg/\\$1,000 of 2005 PPP \\$ </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> other_ghg_ttl </td>\n",
    "        <td> Other GHG emissions, total </td>\n",
    "        <td> KtCO2e </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> ch4_ttl </td>\n",
    "        <td> Methane (CH4) emissions, total </td>\n",
    "        <td> KtCO2 </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> n2o_ttl </td>\n",
    "        <td> Nitrous oxide (N2O) emissions, total </td>\n",
    "        <td> KtCO2 </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> nat_emerg </td>\n",
    "        <td> Droughts, floods, extreme temps </td>\n",
    "        <td> % pop. avg. 1990-2009 </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> pop_urb_aggl_perc </td>\n",
    "        <td> Population in urban agglomerations >1million </td>\n",
    "        <td> % </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> prot_area_perc </td>\n",
    "        <td> Nationally terrestrial protected areas </td>\n",
    "        <td> % of total land area </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> gdp </td>\n",
    "        <td> Gross Domestic Product (GDP) </td>\n",
    "        <td> \\$ </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> gni_per_cap </td>\n",
    "        <td> GNI per capita </td>\n",
    "        <td> Atlas \\$ </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> under_5_mort_rate </td>\n",
    "        <td> Under-five mortality rate </td>\n",
    "        <td> per 1,000 </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> pop_growth_perc </td>\n",
    "        <td> Population growth </td>\n",
    "        <td> annual % </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> pop </td>\n",
    "        <td> Population </td>\n",
    "        <td> 1 </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> urb_pop_growth_perc </td>\n",
    "        <td> Urban population growth </td>\n",
    "        <td> annual % </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> urb_pop </td>\n",
    "        <td> Urban population </td>\n",
    "        <td> 1 </td>\n",
    "    </tr>\n",
    "    \n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_clean2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m      2\u001b[39m chosen_vars = {\u001b[33m'\u001b[39m\u001b[33mCereal yield (kg per hectare)\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mcereal_yield\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      3\u001b[39m                \u001b[33m'\u001b[39m\u001b[33mForeign direct investment, net inflows (\u001b[39m\u001b[38;5;132;01m% o\u001b[39;00m\u001b[33mf GDP)\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mfdi_perc_gdp\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      4\u001b[39m                \u001b[33m'\u001b[39m\u001b[33mAccess to electricity (\u001b[39m\u001b[38;5;132;01m% o\u001b[39;00m\u001b[33mf total population)\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33melec_access_perc\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     22\u001b[39m                \u001b[33m'\u001b[39m\u001b[33mUrban population\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33murb_pop\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     23\u001b[39m                 }\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# rename all variables in the column \"Series name\" with comprehensible shorter versions\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m data_clean2[\u001b[33m'\u001b[39m\u001b[33mSeries name\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mdata_clean2\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mSeries name\u001b[39m\u001b[33m'\u001b[39m].replace(to_replace=chosen_vars)\n",
      "\u001b[31mNameError\u001b[39m: name 'data_clean2' is not defined"
     ]
    }
   ],
   "source": [
    "# define shorter names corresponding to most relevant variables in a dictionary\n",
    "chosen_vars = {'Cereal yield (kg per hectare)': 'cereal_yield',\n",
    "               'Foreign direct investment, net inflows (% of GDP)': 'fdi_perc_gdp',\n",
    "               'Access to electricity (% of total population)': 'elec_access_perc',\n",
    "               'Energy use per units of GDP (kg oil eq./$1,000 of 2005 PPP $)': 'en_per_gdp',\n",
    "               'Energy use per capita (kilograms of oil equivalent)': 'en_per_cap',\n",
    "               'CO2 emissions, total (KtCO2)': 'co2_ttl',\n",
    "               'CO2 emissions per capita (metric tons)': 'co2_per_cap',\n",
    "               'CO2 emissions per units of GDP (kg/$1,000 of 2005 PPP $)': 'co2_per_gdp',\n",
    "               'Other GHG emissions, total (KtCO2e)': 'other_ghg_ttl',\n",
    "               'Methane (CH4) emissions, total (KtCO2e)': 'ch4_ttl',\n",
    "               'Nitrous oxide (N2O) emissions, total (KtCO2e)': 'n2o_ttl',\n",
    "               'Droughts, floods, extreme temps (% pop. avg. 1990-2009)': 'nat_emerg',\n",
    "               'Population in urban agglomerations >1million (%)': 'pop_urb_aggl_perc',\n",
    "               'Nationally terrestrial protected areas (% of total land area)': 'prot_area_perc',\n",
    "               'GDP ($)': 'gdp',\n",
    "               'GNI per capita (Atlas $)': 'gni_per_cap',\n",
    "               'Under-five mortality rate (per 1,000)': 'under_5_mort_rate',\n",
    "               'Population growth (annual %)': 'pop_growth_perc',\n",
    "               'Population': 'pop',\n",
    "               'Urban population growth (annual %)': 'urb_pop_growth_perc',\n",
    "               'Urban population': 'urb_pop'\n",
    "                }\n",
    "\n",
    "# rename all variables in the column \"Series name\" with comprehensible shorter versions\n",
    "data_clean2['Series name'] = data_clean2['Series name'].replace(to_replace=chosen_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## 5. Data frame transformation\n",
    "\n",
    "This is how the current data frame looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_clean2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdata_clean2\u001b[49m.head()\n",
      "\u001b[31mNameError\u001b[39m: name 'data_clean2' is not defined"
     ]
    }
   ],
   "source": [
    "data_clean2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned previously, it is necessary to transform the data frame into a suitable format - the features from the *'Series name'* column into separate columns and the years into a single column. This is done by picking the corresponding values of each feature together with their countries and years, melting these into a single column and then combine them into a new data frame with all features (basing on the same countries and years):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_clean2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# iterate over all chosen features\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m variable \u001b[38;5;129;01min\u001b[39;00m chosen_cols:\n\u001b[32m      9\u001b[39m \n\u001b[32m     10\u001b[39m     \u001b[38;5;66;03m# pick only rows corresponding to the current feature\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     frame = \u001b[43mdata_clean2\u001b[49m[data_clean2[\u001b[33m'\u001b[39m\u001b[33mSeries name\u001b[39m\u001b[33m'\u001b[39m] == variable]\n\u001b[32m     13\u001b[39m     \u001b[38;5;66;03m# melt all the values for all years into one column and rename the columns correspondingly\u001b[39;00m\n\u001b[32m     14\u001b[39m     frame = frame.melt(id_vars=[\u001b[33m'\u001b[39m\u001b[33mCountry code\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mSeries name\u001b[39m\u001b[33m'\u001b[39m]).rename(columns={\u001b[33m'\u001b[39m\u001b[33mCountry code\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mcountry\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mvariable\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33myear\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m'\u001b[39m: variable}).drop([\u001b[33m'\u001b[39m\u001b[33mSeries name\u001b[39m\u001b[33m'\u001b[39m], axis=\u001b[33m'\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'data_clean2' is not defined"
     ]
    }
   ],
   "source": [
    "# save the short feature names into a list of strings\n",
    "chosen_cols = list(chosen_vars.values())\n",
    "\n",
    "# define an empty list, where sub-dataframes for each feature will be saved\n",
    "frame_list = []\n",
    "\n",
    "# iterate over all chosen features\n",
    "for variable in chosen_cols:\n",
    "    \n",
    "    # pick only rows corresponding to the current feature\n",
    "    frame = data_clean2[data_clean2['Series name'] == variable]\n",
    "    \n",
    "    # melt all the values for all years into one column and rename the columns correspondingly\n",
    "    frame = frame.melt(id_vars=['Country code', 'Series name']).rename(columns={'Country code': 'country', 'variable': 'year', 'value': variable}).drop(['Series name'], axis='columns')\n",
    "    \n",
    "    # add the melted dataframe for the current feature into the list\n",
    "    frame_list.append(frame)\n",
    "\n",
    "\n",
    "# merge all sub-frames into a single dataframe, making an outer binding on the key columns 'country','year'\n",
    "from functools import reduce\n",
    "all_vars = reduce(lambda left, right: pd.merge(left, right, on=['country','year'], how='outer'), frame_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this transformation, the new data frame has the following layout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_vars' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mall_vars\u001b[49m.head()\n",
      "\u001b[31mNameError\u001b[39m: name 'all_vars' is not defined"
     ]
    }
   ],
   "source": [
    "all_vars.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## 6. Remove the remaining missing values in an optimal way\n",
    "\n",
    "Although some columns and rows with empty cells have already been deleted, there are still remaining missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check the amount of missing values in each column\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'all_vars' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mcheck the amount of missing values in each column\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mall_vars\u001b[49m.isnull().sum()\n",
      "\u001b[31mNameError\u001b[39m: name 'all_vars' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"check the amount of missing values in each column\")\n",
    "all_vars.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective here is to remove as many empty cells as possible, while preserving the highest possible amount of columns and rows. Missing values can originate from missing data for certain years, countries or features (columns). For this reason, it is necessary to filter the data by these three categories and remove rows or columns with NaNs by starting with the row/column with the highest amount of missing cells. This way to clean the dataset will preserve the maximal amount of possibly valuable information in columns and rows rather than just deleting all rows with any number of missing values.\n",
    "\n",
    "Since there are multiple appearances of the unique values of countries and years, it is necessary to count the number of NaN values for each unique year and country."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Filtering the years by missing values\n",
    "\n",
    "Checking the amount of missing values for each year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_vars' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m all_vars_clean = \u001b[43mall_vars\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m#define an array with the unique year values\u001b[39;00m\n\u001b[32m      4\u001b[39m years_count_missing = \u001b[38;5;28mdict\u001b[39m.fromkeys(all_vars_clean[\u001b[33m'\u001b[39m\u001b[33myear\u001b[39m\u001b[33m'\u001b[39m].unique(), \u001b[32m0\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'all_vars' is not defined"
     ]
    }
   ],
   "source": [
    "all_vars_clean = all_vars\n",
    "\n",
    "#define an array with the unique year values\n",
    "years_count_missing = dict.fromkeys(all_vars_clean['year'].unique(), 0)\n",
    "for ind, row in all_vars_clean.iterrows():\n",
    "    years_count_missing[row['year']] += row.isnull().sum()\n",
    "\n",
    "# sort the years by missing values\n",
    "years_missing_sorted = dict(sorted(years_count_missing.items(), key=lambda item: item[1]))\n",
    "\n",
    "# print the missing values for each year\n",
    "print(\"missing values by year:\")\n",
    "for key, val in years_missing_sorted.items():\n",
    "    print(key, \":\", val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of the filtering is to delete rows with a significant amount of missing values for certain countries without removing too many years. So it is important to choose the proper limit for NaN values allowed per year. The previous output suggests to pick the years between 1991 and 2008 for the further analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of missing values in the whole dataset before filtering the years:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'all_vars_clean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mnumber of missing values in the whole dataset before filtering the years:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mall_vars_clean\u001b[49m.isnull().sum().sum())\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mnumber of rows before filtering the years:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(all_vars_clean.shape[\u001b[32m0\u001b[39m])\n",
      "\u001b[31mNameError\u001b[39m: name 'all_vars_clean' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"number of missing values in the whole dataset before filtering the years:\")\n",
    "print(all_vars_clean.isnull().sum().sum())\n",
    "print(\"number of rows before filtering the years:\")\n",
    "print(all_vars_clean.shape[0])\n",
    "\n",
    "# filter only rows for years between 1991 and 2008 (having less missing values)\n",
    "all_vars_clean = all_vars_clean[(all_vars_clean['year'] >= 1991) & (all_vars_clean['year'] <= 2008)]\n",
    "\n",
    "print(\"number of missing values in the whole dataset after filtering the years:\")\n",
    "print(all_vars_clean.isnull().sum().sum())\n",
    "print(\"number of rows after filtering the years:\")\n",
    "print(all_vars_clean.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Filtering the countries by missing values\n",
    "\n",
    "The same procedure is applied to the filtering of countries with missing values. The following snippet shows the number of NaNs for each country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_vars_clean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# check the amount of missing values by country\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# define an array with the unique country values\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m countries_count_missing = \u001b[38;5;28mdict\u001b[39m.fromkeys(\u001b[43mall_vars_clean\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mcountry\u001b[39m\u001b[33m'\u001b[39m].unique(), \u001b[32m0\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# iterate through all rows and count the amount of NaN values for each country\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ind, row \u001b[38;5;129;01min\u001b[39;00m all_vars_clean.iterrows():\n",
      "\u001b[31mNameError\u001b[39m: name 'all_vars_clean' is not defined"
     ]
    }
   ],
   "source": [
    "# check the amount of missing values by country\n",
    "\n",
    "# define an array with the unique country values\n",
    "countries_count_missing = dict.fromkeys(all_vars_clean['country'].unique(), 0)\n",
    "\n",
    "# iterate through all rows and count the amount of NaN values for each country\n",
    "for ind, row in all_vars_clean.iterrows():\n",
    "    countries_count_missing[row['country']] += row.isnull().sum()\n",
    "\n",
    "# sort the countries by missing values\n",
    "countries_missing_sorted = dict(sorted(countries_count_missing.items(), key=lambda item: item[1]))\n",
    "\n",
    "# print the missing values for each country\n",
    "print(\"missing values by country:\")\n",
    "for key, val in countries_missing_sorted.items():\n",
    "    print(key, \":\", val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This output would suggest to remove rows for countries with more than 90 missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of missing values in the whole dataset before filtering the countries:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'all_vars_clean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mnumber of missing values in the whole dataset before filtering the countries:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mall_vars_clean\u001b[49m.isnull().sum().sum())\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mnumber of rows before filtering the countries:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(all_vars_clean.shape[\u001b[32m0\u001b[39m])\n",
      "\u001b[31mNameError\u001b[39m: name 'all_vars_clean' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"number of missing values in the whole dataset before filtering the countries:\")\n",
    "print(all_vars_clean.isnull().sum().sum())\n",
    "print(\"number of rows before filtering the countries:\")\n",
    "print(all_vars_clean.shape[0])\n",
    "\n",
    "\n",
    "# filter only rows for countries with less than 90 missing values\n",
    "countries_filter = []\n",
    "for key, val in countries_missing_sorted.items():\n",
    "    if val<90:\n",
    "        countries_filter.append(key)\n",
    "\n",
    "all_vars_clean = all_vars_clean[all_vars_clean['country'].isin(countries_filter)]\n",
    "\n",
    "print(\"number of missing values in the whole dataset after filtering the countries:\")\n",
    "print(all_vars_clean.isnull().sum().sum())\n",
    "print(\"number of rows after filtering the countries:\")\n",
    "print(all_vars_clean.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Checking the features (columns) for missing values\n",
    "\n",
    "The NaN values count in each column is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_vars_clean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mall_vars_clean\u001b[49m.isnull().sum()\n",
      "\u001b[31mNameError\u001b[39m: name 'all_vars_clean' is not defined"
     ]
    }
   ],
   "source": [
    "all_vars_clean.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having filtered the years and countries with the most missing values, the features *elec_access_perc*, *other_ghg_ttl*, *ch4_ttl*, *n20_ttl* and *nat_emerg* still contain a significant number of missing values, removing which will reduce the amount of observations very drastically. Therefore, the next step is to remove these columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_vars_clean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mitertools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compress\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# create a boolean mapping of features with more than 20 missing values\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m vars_bad = \u001b[43mall_vars_clean\u001b[49m.isnull().sum()>\u001b[32m20\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# remove the columns corresponding to the mapping of the features with many missing values\u001b[39;00m\n\u001b[32m      9\u001b[39m all_vars_clean2 = all_vars_clean.drop(compress(data = all_vars_clean.columns, selectors = vars_bad), axis=\u001b[33m'\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'all_vars_clean' is not defined"
     ]
    }
   ],
   "source": [
    "# remove features with more than 20 missing values\n",
    "\n",
    "from itertools import compress\n",
    "\n",
    "# create a boolean mapping of features with more than 20 missing values\n",
    "vars_bad = all_vars_clean.isnull().sum()>20\n",
    "\n",
    "# remove the columns corresponding to the mapping of the features with many missing values\n",
    "all_vars_clean2 = all_vars_clean.drop(compress(data = all_vars_clean.columns, selectors = vars_bad), axis='columns')\n",
    "\n",
    "print(\"Remaining missing values per column:\")\n",
    "print(all_vars_clean2.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing the rows with the remainin missing values will not impair the size of the dataset significantly, so these rows will be deleted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_vars_clean2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# delete rows with any number of missing values\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m all_vars_clean3 = \u001b[43mall_vars_clean2\u001b[49m.dropna(axis=\u001b[33m'\u001b[39m\u001b[33mrows\u001b[39m\u001b[33m'\u001b[39m, how=\u001b[33m'\u001b[39m\u001b[33many\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRemaining missing values per column:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(all_vars_clean3.isnull().sum())\n",
      "\u001b[31mNameError\u001b[39m: name 'all_vars_clean2' is not defined"
     ]
    }
   ],
   "source": [
    "# delete rows with any number of missing values\n",
    "all_vars_clean3 = all_vars_clean2.dropna(axis='rows', how='any')\n",
    "\n",
    "print(\"Remaining missing values per column:\")\n",
    "print(all_vars_clean3.isnull().sum())\n",
    "\n",
    "print(\"Final shape of the cleaned dataset:\")\n",
    "print(all_vars_clean3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## 7. Export of the cleaned data frame to a file\n",
    "\n",
    "Now that the dataset has been rearranged and cleaned of missing values, it can be exported to a csv file (without the row index) for further analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_vars_clean3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# export the clean dataframe to a csv file\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mall_vars_clean3\u001b[49m.to_csv(\u001b[33m'\u001b[39m\u001b[33mdata_cleaned.csv\u001b[39m\u001b[33m'\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'all_vars_clean3' is not defined"
     ]
    }
   ],
   "source": [
    "# export the clean dataframe to a csv file\n",
    "all_vars_clean3.to_csv('data_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The further stages of the project are Data Visualization and Predictive Analysis, which can be found in the corresponding notebooks.\n",
    "\n",
    "***\n",
    "\n",
    "Copyright (c) 2020 Vladislav Todorov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
